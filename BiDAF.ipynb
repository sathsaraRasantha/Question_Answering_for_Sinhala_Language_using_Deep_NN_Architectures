{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BiDAF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAhbO/Dlp/eEOWUjOAvpWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathsaraRasantha/Question_Answering_for_Sinhala_Language_using_Deep_NN_Architectures/blob/main/BiDAF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv6OaSNmRyJu",
        "outputId": "f25e22a1-a9e2-4c44-a2a0-0524ed1b1f56"
      },
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import WordPunctTokenizer\n",
        "from nltk import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re, os, string, typing, gc, json\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc-wakVcajOo"
      },
      "source": [
        "# **Build a list of dictionaries with ( context , question , label ) as keys**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWkE_ACWabnr"
      },
      "source": [
        "def parse_data(dataframe):\n",
        "  QA_list=[]\n",
        "\n",
        "  for i in range(0,len(dataframe['index'])):\n",
        "    QA_dict={}\n",
        "    QA_dict['id']=i\n",
        "    QA_dict['context']=dataframe['context_translated'][i]\n",
        "    QA_dict['question']=dataframe['question_translated'][i]\n",
        "    QA_dict['label']=[dataframe['answer_start'][0],dataframe['answer_end'][i]]\n",
        "    QA_list.append(QA_dict)\n",
        "  return QA_list"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-Edh52Vasxs"
      },
      "source": [
        "# **Function to pre-process paragraph of text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIzV3jpGasF4"
      },
      "source": [
        "\n",
        "def preprocess_text(document):\n",
        "        # Remove all the special characters\n",
        "        document = re.sub(r'\\u200d', '', str(document))\n",
        "\n",
        "        # remove all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Remove single characters from the start\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "        # Removing prefixed 'b'\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "        # define punctuation\n",
        "        punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "\n",
        "        preprocessed_text= \"\"\n",
        "        for char in document:\n",
        "          if char not in punctuations:\n",
        "              preprocessed_text = preprocessed_text+ char\n",
        "        \n",
        "        return preprocessed_text"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aAu7qJBbAia"
      },
      "source": [
        "# **Function to create list of Context, Question and Context+Question lists**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiFlvdn8a9OP"
      },
      "source": [
        "def gather_text_for_vocab(df):\n",
        "  context=[]\n",
        "  for row1 in df['context_translated']:\n",
        "    document1=row1\n",
        "    text1=preprocess_text(document1)\n",
        "    context.append(text1)\n",
        "\n",
        "  \n",
        "  question=[]\n",
        "  for row2 in df['question_translated']:\n",
        "    document2=row2\n",
        "    text2=preprocess_text(document2)\n",
        "    question.append(text2)\n",
        "\n",
        "  answer=[]\n",
        "  for row3 in df['text_translated']:\n",
        "    document3=row3\n",
        "    text3=preprocess_text(document3)\n",
        "    answer.append(text3)\n",
        "\n",
        "  lis=context+question\n",
        "  return lis,context,question,answer"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb3HJ8EQbLh5"
      },
      "source": [
        "# **Function to build vocabulary of words , word to index and index to word mapping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNN_zHi_bQpW"
      },
      "source": [
        "def build_word_vocab(lis):\n",
        "   sentence_vocab=[]\n",
        "   for sentences in lis:\n",
        "      tokenized=sentences.split()\n",
        "      sentence_vocab.append(tokenized)\n",
        "   \n",
        "   word2idx = {'__</e>__': 0, '__UNK__': 1} \n",
        "   idx2word={}\n",
        "   word_vocab=[]\n",
        "   for line in lis: \n",
        "     words=line.split()\n",
        "     for word in words:\n",
        "        if word not in word2idx: \n",
        "          word2idx[word] = len(word2idx)\n",
        "        if word not in word_vocab:\n",
        "          word_vocab.append(word)\n",
        "\n",
        "\n",
        "   for item in word2idx:\n",
        "     idx2word[word2idx[item]]=item\n",
        "\n",
        "\n",
        "   return word2idx, idx2word, word_vocab"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE5VGZQAbUVQ"
      },
      "source": [
        "\n",
        "# **Function to create character vocabulary and character to index mapping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQiUmN4sba3q"
      },
      "source": [
        "def build_char_vocab(vocab_text):\n",
        "\n",
        "    chars = []\n",
        "    for sent in vocab_text:\n",
        "        for ch in sent:\n",
        "            chars.append(ch)\n",
        "\n",
        "    char_counter = Counter(chars)\n",
        "    char_vocab = sorted(char_counter, key=char_counter.get, reverse=True)\n",
        "    print(f\"raw-char-vocab: {len(char_vocab)}\")\n",
        "    high_freq_char = [char for char, count in char_counter.items() if count>=20]\n",
        "    char_vocab = list(set(char_vocab).intersection(set(high_freq_char)))\n",
        "    print(f\"char-vocab-intersect: {len(char_vocab)}\")\n",
        "    char_vocab.insert(0,'<unk>')\n",
        "    char_vocab.insert(1,'<pad>')\n",
        "    char2idx = {char:idx for idx, char in enumerate(char_vocab)}\n",
        "    print(f\"char2idx-length: {len(char2idx)}\")\n",
        "    \n",
        "    return char2idx, char_vocab"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl_RZYlAblUz"
      },
      "source": [
        "# **Function to create id's for pre-processed context words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNHHTiQXbkDW"
      },
      "source": [
        "def context_to_ids(text, word2idx):\n",
        "   \n",
        "    context_tokens=text.split()\n",
        "    # for paragraph in stripped:\n",
        "    #     tokenized=word_tokenize(paragraph)\n",
        "    #     context_tokens.append(tokenized)\n",
        "    \n",
        "    context_ids = [word2idx[word] for word in context_tokens]\n",
        "    \n",
        "    assert len(context_ids) == len(context_tokens)\n",
        "    return context_ids"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqEGFPJ4bynX"
      },
      "source": [
        "# **Function to create id's for pre-processed questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJMXtXiNb5E0"
      },
      "source": [
        "def question_to_ids(text, word2idx):\n",
        " \n",
        "\n",
        "    question_tokens=text.split()\n",
        "    question_ids = [word2idx[word] for word in question_tokens]\n",
        "    \n",
        "    assert len(question_ids) == len(question_tokens)\n",
        "    return question_ids"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrUlwrawYvY-"
      },
      "source": [
        "# **Function to create id's for answers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U6plP3XYujw"
      },
      "source": [
        "def answer_to_ids(text, word2idx):\n",
        " \n",
        "\n",
        "    answer_tokens=text.split()\n",
        "    answer_ids = [word2idx[word] for word in answer_tokens]\n",
        "    \n",
        "    assert len(answer_ids) == len(answer_tokens)\n",
        "    return answer_ids"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "QKmDnevXcAiJ",
        "outputId": "ceb78abf-84c6-48ed-907d-02b1b4eb72d2"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e97bb11e-b87b-482a-9165-1e7e268d3559\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e97bb11e-b87b-482a-9165-1e7e268d3559\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving partially_cleaned.csv to partially_cleaned.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "4Wjubr4STig9",
        "outputId": "69f721fc-a37d-488e-fe26-1628fd50d222"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b0c9e552-7469-4304-97cc-2980d83a0e04\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b0c9e552-7469-4304-97cc-2980d83a0e04\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving english_sinhala_combined.csv to english_sinhala_combined.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "BHnC5RwGZkqB",
        "outputId": "d5a504ee-8999-4320-d45a-859b305681ac"
      },
      "source": [
        "data=pd.read_csv(\"english_sinhala_combined.csv\")\n",
        "data=data.drop(['Unnamed: 0',\t'context',\t'question',\t'text'\t],axis=1)\n",
        "data"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_translated</th>\n",
              "      <th>question_translated</th>\n",
              "      <th>text_translated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>උතුරු අත්ලාන්තික් සාගරයේ බොස්ටන්හි වෙරළබඩ පිහි...</td>\n",
              "      <td>බොස්ටන් පිහිටා ඇත්තේ කුමන වෙරළ තීරයේද?</td>\n",
              "      <td>උතුරු අත්ලාන්තික්</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2002 දී ප්‍රදර්ශනය ආරම්භ වූ දා සිට, අයිඩල් ජයග...</td>\n",
              "      <td>ඇමරිකානු අයිඩල් තරඟකරු ක්‍රිස් ඩොට්‍රි කුමන ප්...</td>\n",
              "      <td>උතුරු කැරොලිනාව</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 වන ශතවර්ෂයේ මැද භාගයේ සිට ජර්මානු ගෝත්‍රිකයන...</td>\n",
              "      <td>7 වන සියවසේ ඉදිකරන ලද දේ</td>\n",
              "      <td>රෝම බලකොටුවේ බිත්ති ඇතුළත පල්ලියක් ඉදිකරන ලදි</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>රෝම බලකොටුවේ බිත්ති ඇතුළත පල්ලියක් ඉදිකරන ලදි ...</td>\n",
              "      <td>90 දශකයේ දී කැනේ විසින් සාදන ලද බීට්ස් කවුද?</td>\n",
              "      <td>දේශීය කලාකරුවන්</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ටුවාලුආන් භාෂාව සහ ඉංග්‍රීසි යනු ටුවාලු හි ජාත...</td>\n",
              "      <td>ඉංග්‍රීසි හැරුණු විට ටුවාලු හි ජාතික භාෂාව කුම...</td>\n",
              "      <td>ටුවාලුවාන්</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>දේශපාලන පක්ෂයක අපේක්ෂකයින් පිහිටුවීමට, සාමාජික...</td>\n",
              "      <td>ලිබරල් ප්‍රජාතන්ත්‍රවාදයට රාජ්‍යයක් දක්වන සහයෝ...</td>\n",
              "      <td>පිහිටුවීමට, සාමාජිකත්වය ප්‍රකාශ කිරීමට හෝ උද් ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>පැරණි ඉංග්‍රීසි භාෂාවෙන් ලතින් භාෂාවෙන් යම් නි...</td>\n",
              "      <td>ලතින් හෝඩිය හඳුන්වාදීමට පෙර පැරණි ඉංග්‍රීසි ලි...</td>\n",
              "      <td>රූනික් පද්ධතිය</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>රූනික් පද්ධතිය... ඇරිස්ටෝෆනේස්ගේ නාට්‍ය ග්‍රීක...</td>\n",
              "      <td>ප්ලේටෝ සහ ඇරිස්ටෝටල් ලියුවේ කුමන ආකාරයේ සාහිත්...</td>\n",
              "      <td>දාර්ශනික පා</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>ආරම්භක හා අවසන් දිනයන් දළ වශයෙන් දකුණු අර්ධගෝල...</td>\n",
              "      <td>දකුණු අර්ධගෝලයේ කුමන කාල පරිච්ඡේදයේදී චිලී ප්‍...</td>\n",
              "      <td>ගිම්හානය</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      context_translated  ...                                    text_translated\n",
              "0      උතුරු අත්ලාන්තික් සාගරයේ බොස්ටන්හි වෙරළබඩ පිහි...  ...                                  උතුරු අත්ලාන්තික්\n",
              "1      2002 දී ප්‍රදර්ශනය ආරම්භ වූ දා සිට, අයිඩල් ජයග...  ...                                    උතුරු කැරොලිනාව\n",
              "2      3 වන ශතවර්ෂයේ මැද භාගයේ සිට ජර්මානු ගෝත්‍රිකයන...  ...      රෝම බලකොටුවේ බිත්ති ඇතුළත පල්ලියක් ඉදිකරන ලදි\n",
              "3      රෝම බලකොටුවේ බිත්ති ඇතුළත පල්ලියක් ඉදිකරන ලදි ...  ...                                    දේශීය කලාකරුවන්\n",
              "4      ටුවාලුආන් භාෂාව සහ ඉංග්‍රීසි යනු ටුවාලු හි ජාත...  ...                                         ටුවාලුවාන්\n",
              "...                                                  ...  ...                                                ...\n",
              "19995  දේශපාලන පක්ෂයක අපේක්ෂකයින් පිහිටුවීමට, සාමාජික...  ...  පිහිටුවීමට, සාමාජිකත්වය ප්‍රකාශ කිරීමට හෝ උද් ...\n",
              "19996  පැරණි ඉංග්‍රීසි භාෂාවෙන් ලතින් භාෂාවෙන් යම් නි...  ...                                     රූනික් පද්ධතිය\n",
              "19997  රූනික් පද්ධතිය... ඇරිස්ටෝෆනේස්ගේ නාට්‍ය ග්‍රීක...  ...                                        දාර්ශනික පා\n",
              "19998  ආරම්භක හා අවසන් දිනයන් දළ වශයෙන් දකුණු අර්ධගෝල...  ...                                           ගිම්හානය\n",
              "19999                                                     ...                                                NaN\n",
              "\n",
              "[20000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "F-bs0ABMQfMs",
        "outputId": "b2f07efc-97aa-4f44-ff25-2fa04577ab11"
      },
      "source": [
        "df=pd.read_csv(\"partially_cleaned.csv\")\n",
        "df=df.drop(['Unnamed: 0'],axis=1)\n",
        "df"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>context_translated</th>\n",
              "      <th>question_translated</th>\n",
              "      <th>text_translated</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>උතුරු අත්ලාන්තික් සාගරයේ බොස්ටන්හි වෙරළබඩ පිහි...</td>\n",
              "      <td>බොස්ටන් පිහිටා ඇත්තේ කුමන වෙරළ තීරයේද</td>\n",
              "      <td>උතුරු අත්ලාන්තික්</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>ටුවාලුආන් භාෂාව සහ ඉංග්රීසි යනු ටුවාලු හි ජාති...</td>\n",
              "      <td>ඉංග්රීසි හැරුණු විට ටුවාලු හි ජාතික භාෂාව කුමක්ද</td>\n",
              "      <td>ටුවාලුවාන්</td>\n",
              "      <td>56</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>ජේන් ඔස්ටන් ප්රධාන තදාසන්න ප්රදේශ අතර නිරිත දෙ...</td>\n",
              "      <td>රිච්මන්ඩ් හි නිරිතදිග තදාසන්න ප්රදේශය කුමක්ද</td>\n",
              "      <td>මිඩ්ලෝතියන්</td>\n",
              "      <td>51</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>දෙවන ලෝක යුද්ධයෙන් පසු ජර්මනිය මිත්ර පාක්ෂිකයන...</td>\n",
              "      <td>දෙවන ලෝක යුද්ධයෙන් පසු ජර්මනියේ මුලින් ප්රාන්ත...</td>\n",
              "      <td>හත</td>\n",
              "      <td>326</td>\n",
              "      <td>328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>නැපෝලියන් පශ්චාත් විප්ලවවාදී ප්රංශයේ අවනීතිය ස...</td>\n",
              "      <td>දහස් ගණනකට යුද්ධය හා මරණය පිළිබඳ අපේක්ෂාවෙන් න...</td>\n",
              "      <td>ඔහුගේ විවේචකයෝ</td>\n",
              "      <td>157</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9314</th>\n",
              "      <td>19990</td>\n",
              "      <td>1900 වන විට මිනිසුන් 7531 ක් නගරයේ වාසය කළහ 19...</td>\n",
              "      <td>2006 දී ටක්සන්ගේ ජනගහනය කොපමණද</td>\n",
              "      <td>535000 කි</td>\n",
              "      <td>511</td>\n",
              "      <td>520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9315</th>\n",
              "      <td>19991</td>\n",
              "      <td>තෙවන පරම්පරාව ෆයර්වෙයාර් හෝ යූඑස්බී සම්බන්ධතාව...</td>\n",
              "      <td>ලිපිගොනු මාරු කිරීමේදී ෆයර්වෙයාර් භාවිතය මුලින...</td>\n",
              "      <td>පස්වන</td>\n",
              "      <td>313</td>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9316</th>\n",
              "      <td>19992</td>\n",
              "      <td>ඔරලෝසු මාරුව හෘදයාබාධ ඇතිවීමේ අවදානම සියයට 10 ...</td>\n",
              "      <td>සෘතුමය වෙනස්කම් මගින් කඩාකප්පල් වන ස්වාභාවික ර...</td>\n",
              "      <td>සර්කැඩියානු රිද්මය</td>\n",
              "      <td>131</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9317</th>\n",
              "      <td>19995</td>\n",
              "      <td>දේශපාලන පක්ෂයක අපේක්ෂකයින් පිහිටුවීමට සාමාජිකත...</td>\n",
              "      <td>ලිබරල් ප්රජාතන්ත්රවාදයට රාජ්යයක් දක්වන සහයෝගය ...</td>\n",
              "      <td>පිහිටුවීමට සාමාජිකත්වය ප්රකාශ කිරීමට හෝ උද් ca...</td>\n",
              "      <td>27</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9318</th>\n",
              "      <td>19998</td>\n",
              "      <td>ආරම්භක හා අවසන් දිනයන් දළ වශයෙන් දකුණු අර්ධගෝල...</td>\n",
              "      <td>දකුණු අර්ධගෝලයේ කුමන කාල පරිච්ඡේදයේදී චිලී ප්ර...</td>\n",
              "      <td>ගිම්හානය</td>\n",
              "      <td>304</td>\n",
              "      <td>312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9319 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  ... answer_end\n",
              "0         0  ...         17\n",
              "1         4  ...         66\n",
              "2         8  ...         62\n",
              "3         9  ...        328\n",
              "4        21  ...        171\n",
              "...     ...  ...        ...\n",
              "9314  19990  ...        520\n",
              "9315  19991  ...        318\n",
              "9316  19992  ...        149\n",
              "9317  19995  ...        101\n",
              "9318  19998  ...        312\n",
              "\n",
              "[9319 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "xt07v5VPU1nL",
        "outputId": "a889226a-5f31-4a39-a7eb-84f56f29887a"
      },
      "source": [
        "df=df.rename(columns={\"index\": \"id\", \"context_translated\": \"context\",\"question_translated\":\"question\",\"text_translated\":\"answer\",\"answer_start\":\"start\",\"answer_end\":\"end\"})\n",
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>උතුරු අත්ලාන්තික් සාගරයේ බොස්ටන්හි වෙරළබඩ පිහි...</td>\n",
              "      <td>බොස්ටන් පිහිටා ඇත්තේ කුමන වෙරළ තීරයේද</td>\n",
              "      <td>උතුරු අත්ලාන්තික්</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>ටුවාලුආන් භාෂාව සහ ඉංග්රීසි යනු ටුවාලු හි ජාති...</td>\n",
              "      <td>ඉංග්රීසි හැරුණු විට ටුවාලු හි ජාතික භාෂාව කුමක්ද</td>\n",
              "      <td>ටුවාලුවාන්</td>\n",
              "      <td>56</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>ජේන් ඔස්ටන් ප්රධාන තදාසන්න ප්රදේශ අතර නිරිත දෙ...</td>\n",
              "      <td>රිච්මන්ඩ් හි නිරිතදිග තදාසන්න ප්රදේශය කුමක්ද</td>\n",
              "      <td>මිඩ්ලෝතියන්</td>\n",
              "      <td>51</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>දෙවන ලෝක යුද්ධයෙන් පසු ජර්මනිය මිත්ර පාක්ෂිකයන...</td>\n",
              "      <td>දෙවන ලෝක යුද්ධයෙන් පසු ජර්මනියේ මුලින් ප්රාන්ත...</td>\n",
              "      <td>හත</td>\n",
              "      <td>326</td>\n",
              "      <td>328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>නැපෝලියන් පශ්චාත් විප්ලවවාදී ප්රංශයේ අවනීතිය ස...</td>\n",
              "      <td>දහස් ගණනකට යුද්ධය හා මරණය පිළිබඳ අපේක්ෂාවෙන් න...</td>\n",
              "      <td>ඔහුගේ විවේචකයෝ</td>\n",
              "      <td>157</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9314</th>\n",
              "      <td>19990</td>\n",
              "      <td>1900 වන විට මිනිසුන් 7531 ක් නගරයේ වාසය කළහ 19...</td>\n",
              "      <td>2006 දී ටක්සන්ගේ ජනගහනය කොපමණද</td>\n",
              "      <td>535000 කි</td>\n",
              "      <td>511</td>\n",
              "      <td>520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9315</th>\n",
              "      <td>19991</td>\n",
              "      <td>තෙවන පරම්පරාව ෆයර්වෙයාර් හෝ යූඑස්බී සම්බන්ධතාව...</td>\n",
              "      <td>ලිපිගොනු මාරු කිරීමේදී ෆයර්වෙයාර් භාවිතය මුලින...</td>\n",
              "      <td>පස්වන</td>\n",
              "      <td>313</td>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9316</th>\n",
              "      <td>19992</td>\n",
              "      <td>ඔරලෝසු මාරුව හෘදයාබාධ ඇතිවීමේ අවදානම සියයට 10 ...</td>\n",
              "      <td>සෘතුමය වෙනස්කම් මගින් කඩාකප්පල් වන ස්වාභාවික ර...</td>\n",
              "      <td>සර්කැඩියානු රිද්මය</td>\n",
              "      <td>131</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9317</th>\n",
              "      <td>19995</td>\n",
              "      <td>දේශපාලන පක්ෂයක අපේක්ෂකයින් පිහිටුවීමට සාමාජිකත...</td>\n",
              "      <td>ලිබරල් ප්රජාතන්ත්රවාදයට රාජ්යයක් දක්වන සහයෝගය ...</td>\n",
              "      <td>පිහිටුවීමට සාමාජිකත්වය ප්රකාශ කිරීමට හෝ උද් ca...</td>\n",
              "      <td>27</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9318</th>\n",
              "      <td>19998</td>\n",
              "      <td>ආරම්භක හා අවසන් දිනයන් දළ වශයෙන් දකුණු අර්ධගෝල...</td>\n",
              "      <td>දකුණු අර්ධගෝලයේ කුමන කාල පරිච්ඡේදයේදී චිලී ප්ර...</td>\n",
              "      <td>ගිම්හානය</td>\n",
              "      <td>304</td>\n",
              "      <td>312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9319 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                            context  ... start  end\n",
              "0         0  උතුරු අත්ලාන්තික් සාගරයේ බොස්ටන්හි වෙරළබඩ පිහි...  ...     0   17\n",
              "1         4  ටුවාලුආන් භාෂාව සහ ඉංග්රීසි යනු ටුවාලු හි ජාති...  ...    56   66\n",
              "2         8  ජේන් ඔස්ටන් ප්රධාන තදාසන්න ප්රදේශ අතර නිරිත දෙ...  ...    51   62\n",
              "3         9  දෙවන ලෝක යුද්ධයෙන් පසු ජර්මනිය මිත්ර පාක්ෂිකයන...  ...   326  328\n",
              "4        21  නැපෝලියන් පශ්චාත් විප්ලවවාදී ප්රංශයේ අවනීතිය ස...  ...   157  171\n",
              "...     ...                                                ...  ...   ...  ...\n",
              "9314  19990  1900 වන විට මිනිසුන් 7531 ක් නගරයේ වාසය කළහ 19...  ...   511  520\n",
              "9315  19991  තෙවන පරම්පරාව ෆයර්වෙයාර් හෝ යූඑස්බී සම්බන්ධතාව...  ...   313  318\n",
              "9316  19992  ඔරලෝසු මාරුව හෘදයාබාධ ඇතිවීමේ අවදානම සියයට 10 ...  ...   131  149\n",
              "9317  19995  දේශපාලන පක්ෂයක අපේක්ෂකයින් පිහිටුවීමට සාමාජිකත...  ...    27  101\n",
              "9318  19998  ආරම්භක හා අවසන් දිනයන් දළ වශයෙන් දකුණු අර්ධගෝල...  ...   304  312\n",
              "\n",
              "[9319 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "m1yCSLa9bfQl",
        "outputId": "fb709b54-5e14-4d0b-9f74-ba16a63958b3"
      },
      "source": [
        "label=[]\n",
        "for i in range(0,len(df['id'])):\n",
        "  lis=[df['start'][i],df['end'][i]]\n",
        "  label.append(lis) \n",
        "\n",
        "df['label_idx']=label\n",
        "df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>උතුරු අත්ලාන්තික් සාගරයේ බොස්ටන්හි වෙරළබඩ පිහි...</td>\n",
              "      <td>බොස්ටන් පිහිටා ඇත්තේ කුමන වෙරළ තීරයේද</td>\n",
              "      <td>උතුරු අත්ලාන්තික්</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>[0, 17]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>ටුවාලුආන් භාෂාව සහ ඉංග්රීසි යනු ටුවාලු හි ජාති...</td>\n",
              "      <td>ඉංග්රීසි හැරුණු විට ටුවාලු හි ජාතික භාෂාව කුමක්ද</td>\n",
              "      <td>ටුවාලුවාන්</td>\n",
              "      <td>56</td>\n",
              "      <td>66</td>\n",
              "      <td>[56, 66]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>ජේන් ඔස්ටන් ප්රධාන තදාසන්න ප්රදේශ අතර නිරිත දෙ...</td>\n",
              "      <td>රිච්මන්ඩ් හි නිරිතදිග තදාසන්න ප්රදේශය කුමක්ද</td>\n",
              "      <td>මිඩ්ලෝතියන්</td>\n",
              "      <td>51</td>\n",
              "      <td>62</td>\n",
              "      <td>[51, 62]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>දෙවන ලෝක යුද්ධයෙන් පසු ජර්මනිය මිත්ර පාක්ෂිකයන...</td>\n",
              "      <td>දෙවන ලෝක යුද්ධයෙන් පසු ජර්මනියේ මුලින් ප්රාන්ත...</td>\n",
              "      <td>හත</td>\n",
              "      <td>326</td>\n",
              "      <td>328</td>\n",
              "      <td>[326, 328]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21</td>\n",
              "      <td>නැපෝලියන් පශ්චාත් විප්ලවවාදී ප්රංශයේ අවනීතිය ස...</td>\n",
              "      <td>දහස් ගණනකට යුද්ධය හා මරණය පිළිබඳ අපේක්ෂාවෙන් න...</td>\n",
              "      <td>ඔහුගේ විවේචකයෝ</td>\n",
              "      <td>157</td>\n",
              "      <td>171</td>\n",
              "      <td>[157, 171]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9314</th>\n",
              "      <td>19990</td>\n",
              "      <td>1900 වන විට මිනිසුන් 7531 ක් නගරයේ වාසය කළහ 19...</td>\n",
              "      <td>2006 දී ටක්සන්ගේ ජනගහනය කොපමණද</td>\n",
              "      <td>535000 කි</td>\n",
              "      <td>511</td>\n",
              "      <td>520</td>\n",
              "      <td>[511, 520]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9315</th>\n",
              "      <td>19991</td>\n",
              "      <td>තෙවන පරම්පරාව ෆයර්වෙයාර් හෝ යූඑස්බී සම්බන්ධතාව...</td>\n",
              "      <td>ලිපිගොනු මාරු කිරීමේදී ෆයර්වෙයාර් භාවිතය මුලින...</td>\n",
              "      <td>පස්වන</td>\n",
              "      <td>313</td>\n",
              "      <td>318</td>\n",
              "      <td>[313, 318]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9316</th>\n",
              "      <td>19992</td>\n",
              "      <td>ඔරලෝසු මාරුව හෘදයාබාධ ඇතිවීමේ අවදානම සියයට 10 ...</td>\n",
              "      <td>සෘතුමය වෙනස්කම් මගින් කඩාකප්පල් වන ස්වාභාවික ර...</td>\n",
              "      <td>සර්කැඩියානු රිද්මය</td>\n",
              "      <td>131</td>\n",
              "      <td>149</td>\n",
              "      <td>[131, 149]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9317</th>\n",
              "      <td>19995</td>\n",
              "      <td>දේශපාලන පක්ෂයක අපේක්ෂකයින් පිහිටුවීමට සාමාජිකත...</td>\n",
              "      <td>ලිබරල් ප්රජාතන්ත්රවාදයට රාජ්යයක් දක්වන සහයෝගය ...</td>\n",
              "      <td>පිහිටුවීමට සාමාජිකත්වය ප්රකාශ කිරීමට හෝ උද් ca...</td>\n",
              "      <td>27</td>\n",
              "      <td>101</td>\n",
              "      <td>[27, 101]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9318</th>\n",
              "      <td>19998</td>\n",
              "      <td>ආරම්භක හා අවසන් දිනයන් දළ වශයෙන් දකුණු අර්ධගෝල...</td>\n",
              "      <td>දකුණු අර්ධගෝලයේ කුමන කාල පරිච්ඡේදයේදී චිලී ප්ර...</td>\n",
              "      <td>ගිම්හානය</td>\n",
              "      <td>304</td>\n",
              "      <td>312</td>\n",
              "      <td>[304, 312]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9319 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                            context  ...  end   label_idx\n",
              "0         0  උතුරු අත්ලාන්තික් සාගරයේ බොස්ටන්හි වෙරළබඩ පිහි...  ...   17     [0, 17]\n",
              "1         4  ටුවාලුආන් භාෂාව සහ ඉංග්රීසි යනු ටුවාලු හි ජාති...  ...   66    [56, 66]\n",
              "2         8  ජේන් ඔස්ටන් ප්රධාන තදාසන්න ප්රදේශ අතර නිරිත දෙ...  ...   62    [51, 62]\n",
              "3         9  දෙවන ලෝක යුද්ධයෙන් පසු ජර්මනිය මිත්ර පාක්ෂිකයන...  ...  328  [326, 328]\n",
              "4        21  නැපෝලියන් පශ්චාත් විප්ලවවාදී ප්රංශයේ අවනීතිය ස...  ...  171  [157, 171]\n",
              "...     ...                                                ...  ...  ...         ...\n",
              "9314  19990  1900 වන විට මිනිසුන් 7531 ක් නගරයේ වාසය කළහ 19...  ...  520  [511, 520]\n",
              "9315  19991  තෙවන පරම්පරාව ෆයර්වෙයාර් හෝ යූඑස්බී සම්බන්ධතාව...  ...  318  [313, 318]\n",
              "9316  19992  ඔරලෝසු මාරුව හෘදයාබාධ ඇතිවීමේ අවදානම සියයට 10 ...  ...  149  [131, 149]\n",
              "9317  19995  දේශපාලන පක්ෂයක අපේක්ෂකයින් පිහිටුවීමට සාමාජිකත...  ...  101   [27, 101]\n",
              "9318  19998  ආරම්භක හා අවසන් දිනයන් දළ වශයෙන් දකුණු අර්ධගෝල...  ...  312  [304, 312]\n",
              "\n",
              "[9319 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnIfs_LWSmJ8"
      },
      "source": [
        "msk = np.random.rand(len(df)) < 0.90661835\n",
        "train_df = df[msk]\n",
        "train_df=train_df.reset_index()\n",
        "valid_df = df[~msk]\n",
        "valid_df=valid_df.reset_index()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFlPlVkwS8Mf",
        "outputId": "3275088d-b948-4983-e97d-dfa859702429"
      },
      "source": [
        "print(len(train_df))\n",
        "print(len(valid_df))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8390\n",
            "929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI-NRNIxEg5H"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFHZE1GfERsR"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle, time\n",
        "import re, os, string, typing, gc, json\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "nlp = spacy.load('en')\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX8yD9GFtL9q"
      },
      "source": [
        "**Gather text for create word vocabulary using the full dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vcxFcLoEpFu",
        "outputId": "37e4851a-cf22-4349-e2f2-1496d8bba92f"
      },
      "source": [
        "lis,c,q,a = gather_text_for_vocab(data)\n",
        "\n",
        "vocab_text=lis\n",
        "print(\"Number of sentences in dataset: \", len(vocab_text))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in dataset:  40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeQQaDcFtUqb"
      },
      "source": [
        "**Word-index mapping , index-word mapping , word vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_NxxDcKccyi"
      },
      "source": [
        "word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNaBXNEBthgE"
      },
      "source": [
        "**Character-index mapping , character vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-SCA_edcduh",
        "outputId": "094871bd-f783-4a84-8e04-5f3dfe47d3df"
      },
      "source": [
        "char2idx, char_vocab = build_char_vocab(vocab_text)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw-char-vocab: 794\n",
            "char-vocab-intersect: 191\n",
            "char2idx-length: 193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlWkjoeaty2K"
      },
      "source": [
        "**Creating context and question id's for training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikYIXUNwk3sz"
      },
      "source": [
        "q=0\n",
        "context_ids_train=[]\n",
        "for i in range(0,len(train_df['context'])):\n",
        "    try:                                                        \n",
        "        ids = context_to_ids(train_df['context'][i] , word2idx)\n",
        "        context_ids_train.append(ids)\n",
        "        \n",
        "    except Exception as e:\n",
        "        context_ids_train.append(e)\n",
        "        q=q+1\n",
        "        continue"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mKzOYRVcHXH"
      },
      "source": [
        "r=0\n",
        "question_ids_train=[]\n",
        "for i in range(0,len(train_df['question'])):\n",
        "    try:                                                        \n",
        "        ids = question_to_ids(train_df['question'][i] , word2idx)\n",
        "        question_ids_train.append(ids)\n",
        "        \n",
        "    except Exception as e:\n",
        "        question_ids_train.append(e)\n",
        "        r=r+1\n",
        "        continue"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hXcFG0xZPYY"
      },
      "source": [
        "w=0\n",
        "answer_ids_train=[]\n",
        "for i in range(0,len(train_df['question'])):\n",
        "    try:                                                        \n",
        "        ids = answer_to_ids(train_df['answer'][i] , word2idx)\n",
        "        answer_ids_train.append(ids)\n",
        "        \n",
        "    except Exception as e:\n",
        "        answer_ids_train.append(e)\n",
        "        w=w+1\n",
        "        continue"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdiiCgRXuA09"
      },
      "source": [
        "**Creating context and question id's for validation dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83_2RuTHnyBI"
      },
      "source": [
        "p=0\n",
        "context_ids_valid=[]\n",
        "for i in range(0,len(valid_df['context'])):\n",
        "    try:                                                        \n",
        "        ids = context_to_ids(valid_df['context'][i] , word2idx)\n",
        "        context_ids_valid.append(ids)\n",
        "        \n",
        "    except Exception as e:\n",
        "        context_ids_valid.append(e)\n",
        "        p=p+1\n",
        "        continue"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkag_49xooGG"
      },
      "source": [
        "s=0\n",
        "question_ids_valid=[]\n",
        "for i in range(0,len(valid_df['question'])):\n",
        "    try:                                                        \n",
        "        ids = question_to_ids(valid_df['question'][i] , word2idx)\n",
        "        question_ids_valid.append(ids)\n",
        "        \n",
        "    except Exception as e:\n",
        "        question_ids_valid.append(e)\n",
        "        s=s+1\n",
        "        continue"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIdH7McNZnHC"
      },
      "source": [
        "v=0\n",
        "answer_ids_valid=[]\n",
        "for i in range(0,len(valid_df['question'])):\n",
        "    try:                                                        \n",
        "        ids = answer_to_ids(valid_df['answer'][i] , word2idx)\n",
        "        answer_ids_valid.append(ids)\n",
        "        \n",
        "    except Exception as e:\n",
        "        answer_ids_valid.append(e)\n",
        "        v=v+1\n",
        "        continue"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLNYBjPEuRMl"
      },
      "source": [
        "**Adding context,question id rows to train and valid datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ngTygxmqz3b"
      },
      "source": [
        "train_df['context_ids'] = context_ids_train\n",
        "train_df['question_ids'] = question_ids_train\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-DDqACrW0A"
      },
      "source": [
        "valid_df['context_ids'] = context_ids_valid\n",
        "valid_df['question_ids'] = question_ids_valid\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvkuFFQsryXq"
      },
      "source": [
        "train_df.to_pickle('bidaftrain.pkl')\n",
        "valid_df.to_pickle('bidafvalid.pkl')\n",
        "\n",
        "with open('bidafw2id.pickle','wb') as handle:\n",
        "    pickle.dump(word2idx, handle)\n",
        "\n",
        "with open('bidafc2id.pickle','wb') as handle:\n",
        "    pickle.dump(char2idx, handle)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdSX-zSwCjrI"
      },
      "source": [
        "train_df = pd.read_pickle('bidaftrain.pkl')\n",
        "valid_df = pd.read_pickle('bidafvalid.pkl')\n",
        "\n",
        "with open('bidafw2id.pickle','rb') as handle:\n",
        "    word2idx = pickle.load(handle)\n",
        "with open('bidafc2id.pickle','rb') as handle:\n",
        "    char2idx = pickle.load(handle)\n",
        "\n",
        "idx2word = {v:k for k,v in word2idx.items()}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ox3g94YWMnD"
      },
      "source": [
        "class SquadDataset:\n",
        "    '''\n",
        "    - Creates batches dynamically by padding to the length of largest example\n",
        "      in a given batch.\n",
        "    - Calulates character vectors for contexts and question.\n",
        "    - Returns tensors for training.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, data, batch_size):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
        "        self.data = data\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def make_char_vector(self, max_sent_len, max_word_len, sentence):\n",
        "        \n",
        "        char_vec = torch.ones(max_sent_len, max_word_len).type(torch.LongTensor)\n",
        "        \n",
        "        for i, word in enumerate(sentence.split()):\n",
        "            for j, ch in enumerate(word):\n",
        "                char_vec[i][j] = char2idx.get(ch, 0)\n",
        "        \n",
        "        return char_vec    \n",
        "    \n",
        "    def get_span(self, text):\n",
        "        \n",
        "        text = nlp(text, disable=['parser','tagger','ner'])\n",
        "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
        "\n",
        "        return span\n",
        "\n",
        "    def __iter__(self):\n",
        "        '''\n",
        "        Creates batches of data and yields them.\n",
        "        \n",
        "        Each yield comprises of:\n",
        "        :padded_context: padded tensor of contexts for each batch \n",
        "        :padded_question: padded tensor of questions for each batch \n",
        "        :char_ctx & ques_ctx: character-level ids for context and question\n",
        "        :label: start and end index wrt context_ids\n",
        "        :context_text,answer_text: used while validation to calculate metrics\n",
        "        :ids: question_ids for evaluation\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        for batch in self.data:\n",
        "            \n",
        "            spans = []\n",
        "            ctx_text = []\n",
        "            answer_text = []\n",
        "            \n",
        "            for ctx in batch.context:\n",
        "                ctx_text.append(ctx)\n",
        "                spans.append(self.get_span(ctx))\n",
        "            \n",
        "            for ans in batch.answer:\n",
        "                answer_text.append(ans)\n",
        "                \n",
        "            \n",
        "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
        "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
        "            \n",
        "            for i, ctx in enumerate(batch.context_ids):\n",
        "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
        "\n",
        "            max_word_ctx = 0\n",
        "            for context in batch.context:\n",
        "                for word in nlp(context, disable=['parser','tagger','ner']):\n",
        "                    if len(word.text) > max_word_ctx:\n",
        "                        max_word_ctx = len(word.text)\n",
        "            \n",
        "            char_ctx = torch.ones(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n",
        "            for i, context in enumerate(batch.context):\n",
        "                char_ctx[i] = self.make_char_vector(max_context_len, max_word_ctx, context)\n",
        "            \n",
        "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
        "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
        "            \n",
        "            for i, ques in enumerate(batch.question_ids):\n",
        "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n",
        "                \n",
        "            max_word_ques = 0\n",
        "            for question in batch.question:\n",
        "                for word in nlp(question, disable=['parser','tagger','ner']):\n",
        "                    if len(word.text) > max_word_ques:\n",
        "                        max_word_ques = len(word.text)\n",
        "            \n",
        "            char_ques = torch.ones(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n",
        "            for i, question in enumerate(batch.question):\n",
        "                char_ques[i] = self.make_char_vector(max_question_len, max_word_ques, question)\n",
        "            \n",
        "            ids = list(batch.id)  \n",
        "            label = torch.LongTensor(list(batch.label_idx))\n",
        "            \n",
        "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT-ZvRcyD2G3"
      },
      "source": [
        "train_dataset = SquadDataset(train_df, 16)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCqtoribD6h7"
      },
      "source": [
        "valid_dataset = SquadDataset(valid_df, 16)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwEqt_tD81x",
        "outputId": "c5769469-58b7-45cc-ca70-975ab58c74ac"
      },
      "source": [
        "a = next(iter(train_dataset))\n",
        "a"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[   2,    3,    4,  ...,    1,    1,    1],\n",
              "         [ 255,  256,   15,  ...,    1,    1,    1],\n",
              "         [ 495,  496,  497,  ...,    1,    1,    1],\n",
              "         ...,\n",
              "         [ 287, 1130, 1656,  ...,    1,    1,    1],\n",
              "         [1694, 1695,   98,  ...,    1,    1,    1],\n",
              "         [1766,   98, 1767,  ...,    1,    1,    1]]),\n",
              " tensor([[ 5730,   592,  1194,  2625,  4835, 82141,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [  257,   380,    55,   259,   233,   260,   256,  9417,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [  495,   496,   497,   498,   588,  2623,   563, 30580, 82143,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [  813,   814,  1648,    50,  3111,   341, 18194, 11617, 28808,   821,\n",
              "            354,   812,   198, 15046,     1,     1],\n",
              "         [  898, 75721,  4546,   304,    70,   908, 23829, 82147,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [24802,   172,   149, 63349,  6583,   857,   858, 30580,  4917,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [ 1079,    67,  1057,    69,  1060,  1061,   233,  1075,  1623,  9417,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [40388,   498,  7080,    10,  1448, 15046,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [ 1904,  1934,   276,   367,   865,  9417,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [ 1375,   826,    17,  1376, 60790,  9417,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [ 1105,  1475,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [ 1105,  1475,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [11007,  1552,   330,    50,  1400,  1401,   282,  1553,  1554,    15,\n",
              "           1555,  1530,  1556,    58,  4034, 15046],\n",
              "         [82162, 12370,  2625, 82163,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [ 1694,  1695,    98,  1698,  1699, 13677,  4299,  1063, 32362,     1,\n",
              "              1,     1,     1,     1,     1,     1],\n",
              "         [ 7623,    13,  4299, 22610,  6576,   442,  4678,   386, 38175,  1037,\n",
              "          13291,  9417,     1,     1,     1,     1]]),\n",
              " tensor([[[ 81,   3,  60,  ...,   1,   1,   1],\n",
              "          [ 62,   3,  15,  ...,   1,   1,   1],\n",
              "          [137,  33, 114,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         [[125,  60, 167,  ...,   1,   1,   1],\n",
              "          [ 77,  33, 192,  ...,   1,   1,   1],\n",
              "          [137, 126,   1,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         [[  8,  97, 167,  ...,   1,   1,   1],\n",
              "          [ 46, 179,  50,  ...,   1,   1,   1],\n",
              "          [ 86,  60,   8,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[178,  60,  46,  ...,   1,   1,   1],\n",
              "          [ 46,   3, 185,  ...,   1,   1,   1],\n",
              "          [167, 140, 162,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         [[ 50,  15, 188,  ...,   1,   1,   1],\n",
              "          [165,  27, 139,  ...,   1,   1,   1],\n",
              "          [  8,  44,   1,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         [[ 72, 139,  11,  ...,   1,   1,   1],\n",
              "          [  8,  44,   1,  ...,   1,   1,   1],\n",
              "          [  4, 123, 123,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]]]),\n",
              " tensor([[[ 32, 174, 137,  ...,   1,   1,   1],\n",
              "          [ 43, 185, 126,  ...,   1,   1,   1],\n",
              "          [ 79,   3,  15,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         [[156, 102, 114,  ...,   1,   1,   1],\n",
              "          [126,  36, 188,  ...,   1,   1,   1],\n",
              "          [167, 185, 125,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         [[  8,  97, 167,  ...,   1,   1,   1],\n",
              "          [ 46, 179,  50,  ...,   1,   1,   1],\n",
              "          [ 86,  60,   8,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 86,  60, 162,  ...,  33, 137,  15],\n",
              "          [ 43,  36, 178,  ...,   1,   1,   1],\n",
              "          [ 50,  60, 178,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         [[ 50,  15, 188,  ...,   1,   1,   1],\n",
              "          [165,  27, 139,  ...,   1,   1,   1],\n",
              "          [  8,  44,   1,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
              " \n",
              "         [[  6, 167,  60,  ...,   1,   1,   1],\n",
              "          [ 32, 174, 126,  ...,   1,   1,   1],\n",
              "          [  8,  97, 162,  ...,   1,   1,   1],\n",
              "          ...,\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1],\n",
              "          [  1,   1,   1,  ...,   1,   1,   1]]]),\n",
              " tensor([[  0,  17],\n",
              "         [ 56,  66],\n",
              "         [326, 328],\n",
              "         [157, 171],\n",
              "         [545, 560],\n",
              "         [ 84,  87],\n",
              "         [130, 137],\n",
              "         [177, 194],\n",
              "         [493, 501],\n",
              "         [206, 231],\n",
              "         [  0,   5],\n",
              "         [  0,   5],\n",
              "         [397, 410],\n",
              "         [  5,  10],\n",
              "         [ 14,  33],\n",
              "         [614, 635]]),\n",
              " ['උතුරු අත්ලාන්තික් සාගරයේ බොස්ටන්හි වෙරළබඩ පිහිටීම එහි උෂ්ණත්වය පාලනය කරයි නමුත් බොහෝ හිම සහ වැසි ඇති කළ හැකි නොර් ඊස්ටර් කාලගුණ පද්ධති වලට නගරය බෙහෙවින් නැඹුරු වේ නගරයේ සාමාන්ය වර්ෂයකට අඟල් 438 මිමී 1110 ක වර්ෂාපතනයක් ඇති අතර එක් කන්නයකට අඟල් 438 සෙමී 111 හිම පතනයක් සිදු වේ යමෙකු නගරයෙන් විශේෂයෙන් නගරයට උතුරින් හා බටහිරින් away තට යන විට හිම පතනය නාටකාකාර ලෙස ඉහළ යයි එය සාගරයේ මධ්යස්ථ බලපෑමෙන් away ත්වේ බොහෝ හිම පතනය සිදුවන්නේ දෙසැම්බර් සිට මාර්තු දක්වා වන අතර බොහෝ වසරවල අප්රේල් සහ නොවැම්බර් මාසවල මැනිය හැකි හිම නොමැති අතර මැයි සහ ඔක්තෝබර් මාසවල හිම දුර්ලභ වේ හිම පතනයෙහි ඉහළ සිට වසර දක්වා විචල්යතාවයක් ද ඇත නිදසුනක් ලෙස 201112 ශීත saw තුව දුටුවේ හිම සමුච්චය 93 සෙමී 236 ක් පමණි නමුත් පෙර ශීත තුවේ දී අනුරූප අගය 810 මීටර් 206 විය  ',\n",
              "  'ටුවාලුආන් භාෂාව සහ ඉංග්රීසි යනු ටුවාලු හි ජාතික භාෂා වේ ටුවාලුවාන් යනු පොලිනීසියානු භාෂාවන්හි එලිසියානු කණ්ඩායමට අයත් වන අතර හවායි මාඕරි ටහීටියානු සැමෝවාන් සහ ටොංගන් වැනි අනෙකුත් සියලුම පොලිනීසියානු භාෂාවන්ට දුරස්ථව සම්බන්ධ වේ එය මයික්රොනීසියාවේ සහ උතුරු හා මධ්යම මෙලනෙසියාවේ පොලිනීසියානු වෙළඳසැල්වල කතා කරන භාෂාවන්ට වඩාත්ම සමීපව සම්බන්ධ වේ 19 වන සියවසේ අගභාගයේ සහ 20 වන සියවසේ මුල් භාගයේ ක්රිස්තියානි මිෂනාරිවරුන් ප්රධාන වශයෙන් සමෝවාන් ජාතිකයින් වීම නිසා මෙම භාෂාව සමෝවාන් භාෂාවෙන් ණයට ගෙන ඇත 19 වන සියවසේ අගභාගයේ සහ 20 වන සියවසේ මුල් භාගයේ ක්රිස්තියානි මිෂනාරිවරුන් ප්රධාන වශයෙන් සමෝවාන් ජාතිකයින් වීම නිසා මෙම භාෂාව සමෝවාන් භාෂාවෙන් ණයට ගෙන ඇත',\n",
              "  'දෙවන ලෝක යුද්ධයෙන් පසු ජර්මනිය මිත්ර පාක්ෂිකයන් විසින් අත්පත් කර ගත් කාලය තුළ මිත්ර හමුදා ආන්ඩු විසින් අභ්යන්තර දේශසීමා නැවත සකස් කරන ලදී ජනගහනයෙන් හෝ භූමියෙන් 30 කට වඩා වැඩි තනි රාජ්යයක් සමන්විත නොවේ අතීතයේ දී ප්රුෂියා මෙන් ජර්මනිය තුළ කිසිදු රාජ්යයක් ආධිපත්යය දැරීම වැළැක්වීම මෙහි අරමුණ විය මුලදී යුද්ධයට පෙර පැවති රාජ්යයන් හතක් පමණක් ඉතිරිව තිබුණි බේඩන් අර්ධ වශයෙන් බැවේරියා ප්රමාණයෙන් අඩු බ්රෙමන් හැම්බර්ග් හෙස්සි විශාල කරන ලද සැක්සෝනි සහ තුරින්ගියා රයින්ලන්ඩ්පලටිනේට් නෝර්ත් රයින්වෙස්ට්ෆේලියා සහ සැක්සොනිඅන්හාල්ට් වැනි හයිපනේටඩ් නම් සහිත රාජ්යයන් ඔවුන්ගේ පැවැත්මට බලතල ලබා දිය යුතු අතර ඒවා නිර්මාණය කරනු ලැබුවේ හිටපු ප්රෂියානු පළාත් සහ කුඩා ප්රාන්ත ඒකාබද්ධ කිරීමෙනි ඕඩර්නීස් රේඛාවට නැගෙනහිරින් පිහිටි හිටපු ජර්මානු භූමිය පෝලන්ත හෝ සෝවියට් පරිපාලනය යටතට පත් වූ නමුත් අවම වශයෙන් සංකේතාත්මකව 1960 ගණන්වලදී ස්වෛරීභාවය අත් නොහැරීමට උත්සාහ දරා ඇත කෙසේ වෙතත් එකල බටහිර ජර්මනියේ බල සීමාවෙන් පිටත පිහිටා ඇති බැවින් මෙම භූමි ප්රදේශවල නව රාජ්යයන් පිහිටුවීමට කිසිදු උත්සාහයක් ගෙන නොමැත කෙසේ වෙතත් එකල බටහිර ජර්මනියේ අධිකරණ බලයෙන් පිටත පිහිටා ඇති බැවින් මෙම භූමි ප්රදේශවල නව රාජ්යයන් පිහිටුවීමට කිසිදු උත්සාහයක් ගෙන නොමැත',\n",
              "  'නැපෝලියන් පශ්චාත් විප්ලවවාදී ප්රංශයේ අවනීතිය සහ ව්යාකූලත්වය අවසන් කළේය කෙසේවෙතත් ඔහුගේ විරුද්ධවාදීන් විසින් ඔහු ක ර පාලකයෙකු හා කොල්ලකරුවෙකු ලෙස සලකනු ලැබීය ඔහුගේ විවේචකයෝ චෝදනා කරන්නේ දහස් ගණනකට යුද්ධයේ හා මරණයේ අපේක්ෂාවන්ට මුහුණ දෙන විට ඔහු සැලකිය යුතු ලෙස කරදරයට පත් නොවූ බවත් අවිවාදිත පාලනය සෙවීම යුරෝපය පුරා ගැටුම් මාලාවක් බවට පත් කළ බවත් ගිවිසුම් හා සම්මුතීන් නොසලකා හැර ඇති බවත්ය හයිටි විප්ලවය තුළ ඔහුගේ භූමිකාව සහ ප්රංශයේ අධීක්ෂණ ජනපදවල වහල්භාවය නැවත ස්ථාපිත කිරීමට ගත් තීරණය මතභේදාත්මක වන අතර ඔහුගේ කීර්තිනාමයට එය බලපායි හයිටි විප්ලවය තුළ ඔහුගේ භූමිකාව සහ ප්රංශයේ විදේශීය ජනපදවල වහල්භාවය නැවත ස්ථාපිත කිරීමට ගත් තීරණය මතභේදාත්මක වන අතර ඔහුගේ කීර්තිනාමයට එය බලපායි',\n",
              "  'නියපොතු විලාසිතාවේ ආමන්ඩ් හැඩැති සිරුරක් බඳුනකට සමාන වන අතර එය වක්රාකාර ලී වලින් සාදා ඇත එය සාමාන්යයෙන් නැමුණු ශබ්ද වගුවක් ඇති අතර එය ගුවන් යානා දෙකකින් සමන්විත වන අතර පා මාලා හතරකින් සකස් කර ඇති ලෝහ නූල් 8 හි ආතතිය සැලකිල්ලට ගනී Wood න දැව ඇඟිලි පුවරුවක් ඉහළින් වාඩි වී හෝ ශබ්ද වගුව සමඟ ෆ්ලෂ් කර ඇත ඉතා පැරණි උපකරණ ලී සුසර කූ gs ් use භාවිතා කළ හැකි අතර නවතම උපකරණ සන්නද්ධ ලෝහ ටියුනර් භාවිතා කරයි පාලම යනු දැවමය චලනය කළ හැකි දිගකි පික්ගාර්ඩ් එකක් නූල් යට ශබ්ද කුහරයට පහළින් ඇලී තිබේ යුරෝපීය වට රවුම සාමාන්යයෙන් ආචොප් මැන්ඩොලින් වල 13876 වෙනුවට අඟල් 13 පරිමාණයක් භාවිතා කරයි යුරෝපීය වට රවුම සාමාන්යයෙන් ආචොප් මැන්ඩොලින් වල 13876 වෙනුවට අඟල් 13 පරිමාණයක් භාවිතා කරයි',\n",
              "  'Amazoncom නාගුමෝට ඔහුගේ ගුවන් යානා හතරෙන් එක්සත් ජනපද 348 ගොඩබිම 115 සිට ගුවන් යානා 272 ක් ක්රියාත්මක විය',\n",
              "  '2010 සංගණනයට අනුව සැන් ඩියාගෝ නගරයේ 1307402 ක් ජීවත් වූහ 2000 දී වාර්තා වූ 1223400 ක් කුටුම්භ 450691 ක් සහ පවුල් 271315 ක් අතරින් 7 ට අඩු ජනගහන වැඩිවීමක් එයින් නියෝජනය වේ 2009 දී ඇස්තමේන්තුගත නගර ජනගහනය 1306300 කි ජන dens නත්වය වර්ග සැතපුමකට 37719 ක් විය 14564  km2 සැන් ඩියාගෝ හි වාර්ගික සැකැස්ම 451 සුදු 67 අප්රිකානු ඇමරිකානු 06 ස්වදේශික ඇමරිකානු 159 ආසියානු 59 පිලිපීන 27 චීන 25 වියට්නාම 13 ඉන්දියානු 10 කොරියානු 07 ජපන් 04  ලාඕටියානු 03 කාම්බෝජ 01 තායි 05 පැසිෆික් දූපත් වැසියන් 02 ග්වාමියානු 01 සැමෝවාන් 01 ස්වදේශික හවායි වෙනත් තරඟ වලින් 123 සහ තරඟ දෙකකින් හෝ වැඩි ගණනකින් 51 නගරයේ ජනවාර්ගික සැකැස්ම 288 හිස්පැනික් හෝ ලතින් ඕනෑම ජාතියකට මුළු ජනගහනයෙන් 249 ක් මෙක්සිකානු ඇමරිකානුවන් වන අතර 06 ක් පුවර්ටෝ රිකන් ය නගරයේ ජනවාර්ගික සැකැස්ම 288 හිස්පැනික් හෝ ලතින් ඕනෑම ජාතියකට මුළු ජනගහනයෙන් 249 ක් මෙක්සිකානු ඇමරිකානුවන් වන අතර 06 ක් පුවර්ටෝ රිකන් ය',\n",
              "  '2012 අප්රියෙල් 12 වන දින සවස රටේ හමුදා සාමාජිකයින් කුමන්ත්රණයක් දියත් කර අන්තර්වාර ජනාධිපතිවරයා සහ ප්රමුඛ ජනාධිපති අපේක්ෂකයෙකු අත්අඩංගුවට ගත්හ හිටපු උප මාණ්ඩලික ප්රධානී ජෙනරාල් මාමඩු ටුරේ කුරුමා සංක්රාන්ති සමයේදී රටේ පාලනය භාරගත් අතර විරුද්ධ පක්ෂ සමඟ සාකච්ඡා ආරම්භ කළේය හිටපු උප මාණ්ඩලික ප්රධානී ජෙනරාල් මාමඩු ටුරේ කුරුමා සංක්රාන්ති සමයේදී රටේ පාලනය භාරගත් අතර විරුද්ධ පක්ෂ සමඟ සාකච්ඡා ආරම්භ කළේය',\n",
              "  'චෙක්හි මූලික ස්වර දුරකථන 10 ක් අඩංගු වන අතර තවත් තුනක් ණය වචන වලින් පමණක් සොයාගත හැකිය ඒවා    ɛ   ɪ    සහ   ඔවුන්ගේ දිගු සගයන්  aː   ɛː   iː   oː  සහ  uː  සහ ඩිප්තොං තුනක්  ou̯   au̯  සහ  ɛu̯  දෙවැන්න ඩිප්තොං දෙක සහ දිගු  oː  ණය වචන සඳහා පමණක් සීමා වේ අවධාරණය නොකෙරෙන විට ස්වර කිසි විටෙකත් ෂ්වා ශබ්දවලට අඩු නොවේ සංකේතාත්මක සුළු මොනොසයිලාබික් නොකැඩූ අක්ෂර හැරුණු විට සෑම වචනයක්ම එහි පළමු අක්ෂරයට මූලික ආතතියක් ඇත අක්ෂර දෙකකට වඩා වැඩි වචන වලින් සෑම අමුතු අංකයක් සඳහාම ද්විතියික ආතතිය ලැබේ ආතතිය ස්වර දිගට සම්බන්ධ නොවන අතර කෙටි ස්වර සහ අවධාරණය නොකළ දිගු ස්වර ඇතිවීමේ හැකියාව ස්වදේශීය භාෂාවෙන් ඉංග්රීසි වැනි සංයෝජනය වන සිසුන්ට ව්යාකූල විය හැකිය ආතතිය ස්වර දිගට සම්බන්ධ නොවන අතර කෙටි ස්වර සහ අවධාරණය නොකළ දිගු ස්වර ඇතිවීමේ හැකියාව ස්වදේශීය භාෂාවෙන් ඉංග්රීසි වැනි සංයෝජනය වන සිසුන්ට ව්යාකූල විය හැකිය',\n",
              "  'ඉන්ලයින් හොකී ප්රථම වරට ක්රීඩා කරනු ලැබුවේ 1995 දී වන අතර පසුගිය වසරවල දී එය වඩාත් ජනප්රිය වී තිබේ 2008 FIRS ලෝක ශූරතාවලියට කාන්තා පේළිගත හොකී ජාතික කණ්ඩායම සහභාගී විය නැමීබියාව යනු ලොව දුෂ්කරම අඩිපාරක් වන නැමීබියානු අල්ට්රා මැරතන් තරඟයයි නැමීබියාවේ වඩාත් ජනප්රිය මලල ක්රීඩකයා වන්නේ ෆ්රැන්කි ෆ්රෙඩ්රික්ස් ස්ප්රින්ටර් මීටර 100 සහ 200 ය ඔහු ඔලිම්පික් රිදී පදක්කම් 4 ක් 1992 1996 දිනා ඇති අතර ලෝක මලල ක්රීඩා ශූරතා කිහිපයකින් පදක්කම් ද ලබා ඇත ඔහු නැමීබියාවේ සහ ඉන් ඔබ්බෙහි මානුෂීය ක්රියාකාරකම් සඳහා ද ප්රසිද්ධය ඔහු නැමීබියාවේ සහ ඉන් ඔබ්බෙහි මානුෂීය ක්රියාකාරකම් සඳහා ද ප්රසිද්ධය',\n",
              "  '25 යි 25 යි',\n",
              "  '25 යි 25 යි',\n",
              "  'ප්රධාන ධාරාවේ ඒසී වසර ගණනාවක් පුරා සමාන ආකාරයකින් පරිණාමය වී ඇත සාම්ප්රදායික ඒසී කලාකරුවන් වන බාබ්රා ස්ට්රයිසැන්ඩ් වඩු කාර්මිකයන් ඩයෝන් වෝර්වික් බැරී මැනිලෝ ජෝන් ඩෙන්වර් සහ ඔලිවියා නිව්ටන්ජෝන් 1980 දශකය පුරා පැළඳ සිටි ප්රධාන ප්රධාන ගීත 40 ලබා ගැනීම දුෂ්කර වූ අතර එම්ටීවී හි බලපෑම හේතුවෙන් කලාකරුවන් සමකාලීන හිට් රේඩියෝ ආකෘතියේ ප්රධාන කොටස් වන රිචඩ් මාක්ස් මයිකල් ජැක්සන් බොනී ටයිලර් ජෝර්ජ් මයිකල් ෆිල් කොලින්ස් සහ ලෝරා බ්රැනිගන් වැඩි සංඛ්යාතයකින් AC ප්රස්ථාර වෙත ගමන් කිරීමට පටන් ගත්හ ඕල් මියුසික් විසින් කොලින්ස් විස්තර කර ඇත්තේ 80 දශකයේ හා ඉන් ඔබ්බෙහි වඩාත් සාර්ථක පොප් සහ වැඩිහිටි සමකාලීන ගායකයෙකු ලෙස ය කෙසේ වෙතත් එම්ටීවී සහ ඒසී ගුවන්විදුලි සංයෝජනය සමඟ වැඩිහිටි සමකාලීනය ප්රභේදයක් ලෙස අර්ථ දැක්වීමට අසීරු වූ අතර අතීතයේ ස්ථාපිත මෘදුරොක් කලාකරුවන් තවමත් පොප් පහරවල් සටහන් කර ඇති අතර එකල නවක කලාකරුවන්ගෙන් ප්රධාන ධාරාවේ ගුවන් විදුලි ගාස්තු සමඟ ගුවන් වාදනය ද ලබා ගත්හ කෙසේ වෙතත් එම්ටීවී සහ ඒසී ගුවන්විදුලි සංයෝජනය සමඟ වැඩිහිටි සමකාලීනය ප්රභේදයක් ලෙස අර්ථ දැක්වීමට අසීරු වූ අතර අතීතයේ ස්ථාපිත මෘදුරොක් කලාකරුවන් තවමත් පොප් පහරවල් සටහන් කර ඇති අතර එකල නවක කලාකරුවන්ගෙන් ප්රධාන ධාරාවේ ගුවන් විදුලි ගාස්තු සමඟ ගුවන් වාදනය ද ලබා ගත්හ',\n",
              "  'මුල් ලතින් වචනය වන “යුනිවර්සිටාස්” පොදුවේ “එක් ශරීරයක් සමාජයක් සමාගමක් ප්රජාවක් ගිල්ඩ් සංස්ථාවක් යනාදිය හා සම්බන්ධ පුද්ගලයින් ගණනාවක්” යන්නයි නාගරික නගර ජීවිතය හා මධ්යකාලීන සංසදයන් බිහිවන අවස්ථාවේ දී විශේෂිත “සාමූහික නෛතික අයිතිවාසිකම් ඇති සිසුන්ගේ සහ ගුරුවරුන්ගේ සංගම් සාමාන්යයෙන් කුමාරවරුන් නාහිමිවරුන් හෝ ඔවුන් සිටි නගර විසින් නිකුත් කරන ලද ප්ර ters ප්ති මගින් සහතික කරනු ලැබේ” සාමාන්ය පදය අනෙකුත් සංසදවල මෙන් ඔවුන් ස්වයංනියාමනය කරන ලද අතර ඔවුන්ගේ සාමාජිකයන්ගේ සුදුසුකම් තීරණය කළහ අනෙකුත් සංසදවල මෙන් ඔවුන් ස්වයංනියාමනය කරන ලද අතර ඔවුන්ගේ සාමාජිකයන්ගේ සුදුසුකම් තීරණය කළහ',\n",
              "  'ක්රිපූ 480 දී මිලියන 50 ක ජනතාවක් අචෙමනිඩ් අධිරාජ්යයේ ජීවත් වූ බවට ගණන් බලා තිබේ අධිරාජ්යය උච්චතම අවස්ථාවේ දී ලෝක ජනගහනයෙන් 44 කට වඩා පාලනය කළ අතර එය ඉතිහාසයේ ඕනෑම අධිරාජ්යයක් සඳහා වූ ඉහළම අගයයි ග්රීක ඉතිහාසයේ බබිලෝනියේ යුදෙව් වහලුන් ඇතුළු වහලුන්ගේ විමුක්තිය මාර්ග හා තැපැල් පද්ධති වැනි යටිතල පහසුකම් ගොඩනැගීම සහ නිල භාෂාවක් භාවිතා කිරීම සඳහා ග්රීක නගර රාජ්යයන්ගේ විරුද්ධවාදියා ලෙස අචෙමනිඩ් අධිරාජ්යය සැලකේ  එහි භූමි ප්රදේශය පුරා අධිරාජ්යයා යටතේ අධිරාජ්යයා යටතේ කේන්ද්රීය නිලධාරිවාදී පරිපාලනයක් විශාල වෘත්තීය හමුදාවක් සහ සිවිල් සේවා පැවතුණි තවද පුරාණ ලෝකයේ පුදුම හතෙන් එකක් වන හැලිකාර්නාසස්හි සොහොන් ගෙය ක්රිපූ 353 ත් 350 ත් අතර අධිරාජ්යයේ ඉදිකරන ලද්දකි තවද පුරාණ ලෝකයේ පුදුම හතෙන් එකක් වන හැලිකාර්නාසස්හි සොහොන් ගෙය ක්රිපූ 353 ත් 350 ත් අතර අධිරාජ්යයේ ඉදිකරන ලද්දකි',\n",
              "  '2014 දී FAA විසින් ගුවන් ගමන් පාලක අපේක්ෂකයින් සඳහා දීර් approach කාලීන ප්රවේශයක් වෙනස් කළ අතර එය අත්දැකීම් නොසලකා ඕනෑම කෙනෙකුට විවෘත වන පෞරුෂත්ව පරීක්ෂණයකට පක්ෂව ගුවන් ගමන් පාසල්වල පුහුණුව හා පළපුරුද්ද මත පදනම් වූ මනාපයන් ඉවත් කළේය ගුවන් ගමන් රථවාහන පාලක වාර්ගික විවිධත්වය ඉහළ නැංවීම සඳහා මෙම පියවර ගෙන තිබේ වෙනස් වීමට පෙර සහභාගී වන විද්යාලවල සහ විශ්ව විද්යාලවල පා work මාලා සම්පූර්ණ කළ අපේක්ෂකයින් සලකා බැලීම සඳහා වේගයෙන් ලුහුබැඳිය හැකිය ඒජන්සිය එම වැඩසටහන ඉවත් කර ඒ වෙනුවට කිසිදු අත්දැකීමක් හෝ විශ්ව විද්යාල උපාධියක් අවශ්ය නොවී විවෘත පද්ධතියකට සාමාන්ය ජනතාව වෙත මාරු විය ඒ වෙනුවට බොහෝ අයදුම්කරුවන්ට බාධා ඇති වූ චරිතාපදාන ප්රශ්නාවලියක් අයදුම්කරුවන්ට ගත හැකිය ඒ වෙනුවට බොහෝ අයදුම්කරුවන්ට බාධා ඇති වූ චරිතාපදාන ප්රශ්නාවලියක් අයදුම්කරුවන්ට ගත හැකිය'],\n",
              " ['උතුරු අත්ලාන්තික්',\n",
              "  'ටුවාලුවාන්',\n",
              "  'හත',\n",
              "  'ඔහුගේ විවේචකයෝ',\n",
              "  'අඟල් 13 පරිමාණය',\n",
              "  '272',\n",
              "  '7 ට අඩු',\n",
              "  'මාමඩු ටුරේ කුරුමා',\n",
              "  'ස්වර දිග',\n",
              "  'නැමීබියානු අල්ට්රා මැරතන්',\n",
              "  '25 යි',\n",
              "  '25 යි',\n",
              "  'ෆිල් කොලින්ස්',\n",
              "  'ලතින්',\n",
              "  'මිලියන 50 ක ජනතාවක්',\n",
              "  'චරිතාපදාන ප්රශ්නාවලිය'],\n",
              " [0, 4, 9, 21, 22, 26, 27, 28, 37, 38, 41, 42, 45, 52, 53, 56])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zIrhAA2vcS4"
      },
      "source": [
        "# **Start of layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk0BJEFAsHsF"
      },
      "source": [
        "# !pip install PyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7OMaKJpsR1e"
      },
      "source": [
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPkf79YxsWQh"
      },
      "source": [
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPyMRWJjsi7I"
      },
      "source": [
        "# downloaded = drive.CreateFile({'id':\"1Rr1mgeyu7V7xv5Z5CgYsPVzaTBfWZ08s\"})  \n",
        "# downloaded.GetContentFile('cc.si.300.bin')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhetKqPSsyc9",
        "outputId": "1022ae25-1e2e-4b1c-a147-ef6983358db7"
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 19.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.19.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3040939 sha256=dc21989e569f422f7eb6f6285e72368af676d47ab6e60c7652281c819c52e3cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T351JQLqsmpL",
        "outputId": "eaeec54f-8531-4377-b150-fecbe0b5b85b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO-VmEDkthc5",
        "outputId": "066f462e-798f-4545-a3b4-9dccc09683d1"
      },
      "source": [
        "import fasttext\n",
        "dir=\"drive/My Drive/Word_Embeddings/\"\n",
        "model_fastText=fasttext.load_model(\"drive/My Drive/Word_Embeddings/Copy of cc.si.300.bin\", )"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G02hSvgFbB8i"
      },
      "source": [
        "# with open(\"cc.si.300.vec\", \"r\") as f:\n",
        "#     array = []\n",
        "#     for line in f:\n",
        "#         array.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aObbD8XTth5M"
      },
      "source": [
        "\n",
        "# def get_fastText_dict():\n",
        "#     '''\n",
        "#     Parses the glove word vectors text file and returns a dictionary with the words as\n",
        "#     keys and their respective pretrained word vectors as values.\n",
        "\n",
        "#     '''\n",
        "#     fastText_dict = {}\n",
        "#     with open(\"cc.si.300.vec\", \"r\", encoding=\"utf-8\") as f:\n",
        "#         for line in f:\n",
        "#             values = line.split()\n",
        "#             word = values[0]\n",
        "#             vector = np.asarray(values[1:], \"float32\")\n",
        "#             fastText_dict[word] = vector\n",
        "            \n",
        "#     f.close()\n",
        "    \n",
        "#     return fastText_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVA0MEVSt1fr"
      },
      "source": [
        "# fastText_dict = get_fastText_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8N8e2q_ouhk",
        "outputId": "7d31e96f-8a1a-41fe-945f-240629e0cebe"
      },
      "source": [
        "# type(fastText_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoJ7MP-knJCK"
      },
      "source": [
        "\n",
        "# def create_weights_matrix(fastText_dict):\n",
        "#     '''\n",
        "#     Creates a weight matrix of the words that are common in the fastText vocab and\n",
        "#     the dataset's vocab. Initializes OOV words with a zero vector.\n",
        "#     '''\n",
        "#     weights=[]\n",
        "#     #weights_matrix = np.zeros((len(word_vocab), 300))\n",
        "#     words_found = 0\n",
        "#     for word in word_vocab:\n",
        "#         # try:\n",
        "#           w=fastText_dict[word]\n",
        "#           weights.append(w)\n",
        "\n",
        "#         #weights_matrix[i] = fastText_dict[word]\n",
        "#           words_found += 1\n",
        "#         # except:\n",
        "#         #    pass\n",
        "        \n",
        "#     # return weights_matrix, words_found\n",
        "#     return weights,words_found"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOXzKeUdnb0P"
      },
      "source": [
        "def create_weights_matrix():\n",
        "    '''\n",
        "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
        "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
        "    '''\n",
        "    weights_matrix = np.zeros((len(word_vocab), 300))\n",
        "    words_found = 0\n",
        "    for i, word in enumerate(word_vocab):\n",
        "        try:\n",
        "            weights_matrix[i] = model_fastText[word]\n",
        "            words_found += 1\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    return weights_matrix, words_found"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKilu5BIh-8F",
        "outputId": "d0583d71-ddc8-4425-db1f-d06bf3e3873a"
      },
      "source": [
        "\n",
        "weights_matrix, words_found = create_weights_matrix()\n",
        "print(\"Words found in the GloVe vocab: \" ,words_found)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words found in the GloVe vocab:  85812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2QXTP46lGFY"
      },
      "source": [
        "np.save('bidafglove.npy', weights_matrix)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syJfUu5Ln5pA",
        "outputId": "64528e61-3263-4647-dafe-52ca9c82de8b"
      },
      "source": [
        "len(word_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85812"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxpRIcKdahuZ"
      },
      "source": [
        "from torch import nn\n",
        "import torch"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiHLvr1zWVwz"
      },
      "source": [
        "class CharacterEmbeddingLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.char_emb_dim = char_emb_dim\n",
        "        \n",
        "        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1)\n",
        "        \n",
        "        self.char_convolution = nn.Conv2d(in_channels=1, out_channels=300, kernel_size=kernel_size)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    def forward(self, x):\n",
        "        # x = [bs, seq_len, word_len]\n",
        "        # returns : [batch_size, seq_len, num_output_channels]\n",
        "        # the output can be thought of as another feature embedding of dim 100.\n",
        "        \n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        x = self.dropout(self.char_embedding(x))\n",
        "        # x = [bs, seq_len, word_len, char_emb_dim]\n",
        "        \n",
        "        # following three operations manipulate x in such a way that\n",
        "        # it closely resembles an image. this format is important before \n",
        "        # we perform convolution on the character embeddings.\n",
        "        \n",
        "        x = x.permute(0,1,3,2)\n",
        "        # x = [bs, seq_len, char_emb_dim, word_len]\n",
        "        \n",
        "        x = x.view(-1, self.char_emb_dim, x.shape[3])\n",
        "        # x = [bs*seq_len, char_emb_dim, word_len]\n",
        "        \n",
        "        x = x.unsqueeze(1)\n",
        "        # x = [bs*seq_len, 1, char_emb_dim, word_len]\n",
        "        \n",
        "        # x is now in a format that can be accepted by a conv layer. \n",
        "        # think of the tensor above in terms of an image of dimension\n",
        "        # (N, C_in, H_in, W_in).\n",
        "        \n",
        "        x = self.relu(self.char_convolution(x))\n",
        "        # x = [bs*seq_len, out_channels, H_out, W_out]\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        # x = [bs*seq_len, out_channels, W_out]\n",
        "                \n",
        "        x = F.max_pool1d(x, x.shape[2]).squeeze()\n",
        "        # x = [bs*seq_len, out_channels, 1] => [bs*seq_len, out_channels]\n",
        "                \n",
        "        x = x.view(batch_size, -1, x.shape[-1])\n",
        "        # x = [bs, seq_len, out_channels]\n",
        "        # x = [bs, seq_len, features] = [bs, seq_len, 100]\n",
        "        \n",
        "        \n",
        "        return x"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW99fqf7bAm0"
      },
      "source": [
        "class HighwayNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, num_layers=2):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
        "        self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            \n",
        "            flow_value = F.relu(self.flow_layer[i](x))\n",
        "            gate_value = torch.sigmoid(self.gate_layer[i](x))\n",
        "            \n",
        "            x = gate_value * flow_value + (1-gate_value) * x\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGTDINYZc9Lb"
      },
      "source": [
        "class ContextualEmbeddingLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        \n",
        "        self.highway_net = HighwayNetwork(input_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x = [bs, seq_len, input_dim] = [bs, seq_len, emb_dim*2]\n",
        "        # the input is the concatenation of word and characeter embeddings\n",
        "        # for the sequence.\n",
        "        \n",
        "        highway_out = self.highway_net(x)\n",
        "        # highway_out = [bs, seq_len, input_dim]\n",
        "        \n",
        "        outputs, _ = self.lstm(highway_out)\n",
        "        # outputs = [bs, seq_len, emb_dim*2]\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtDD5KtxdAgQ"
      },
      "source": [
        "class BiDAF(nn.Module):\n",
        "    \n",
        "    def __init__(self, char_vocab_dim, emb_dim, char_emb_dim, num_output_channels, \n",
        "                 kernel_size, ctx_hidden_dim, device):\n",
        "        '''\n",
        "        char_vocab_dim = len(char2idx)\n",
        "        emb_dim = 100\n",
        "        char_emb_dim = 8\n",
        "        num_output_chanels = 100\n",
        "        kernel_size = (8,5)\n",
        "        ctx_hidden_dim = 100\n",
        "        '''\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.word_embedding = self.get_glove_embedding()\n",
        "        \n",
        "        self.character_embedding = CharacterEmbeddingLayer(char_vocab_dim, char_emb_dim, \n",
        "                                                      num_output_channels, kernel_size)\n",
        "        \n",
        "        self.contextual_embedding = ContextualEmbeddingLayer(emb_dim*2, ctx_hidden_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout()\n",
        "        \n",
        "        self.similarity_weight = nn.Linear(emb_dim*6, 1, bias=False)\n",
        "        \n",
        "        self.modeling_lstm = nn.LSTM(emb_dim*8, emb_dim, bidirectional=True, num_layers=2, batch_first=True, dropout=0.2)        \n",
        "        self.output_start = nn.Linear(emb_dim*10, 1, bias=False)\n",
        "        \n",
        "        self.output_end = nn.Linear(emb_dim*10, 1, bias=False)\n",
        "        \n",
        "        self.end_lstm = nn.LSTM(emb_dim*2, emb_dim, bidirectional=True, batch_first=True)\n",
        "        \n",
        "    \n",
        "    def get_glove_embedding(self):\n",
        "        \n",
        "        weights_matrix = np.load('bidafglove.npy')\n",
        "        num_embeddings, embedding_dim = weights_matrix.shape\n",
        "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
        "\n",
        "        return embedding\n",
        "        \n",
        "    def forward(self, ctx, ques, char_ctx, char_ques):\n",
        "        # ctx = [bs, ctx_len]\n",
        "        # ques = [bs, ques_len]\n",
        "        # char_ctx = [bs, ctx_len, ctx_word_len]\n",
        "        # char_ques = [bs, ques_len, ques_word_len]\n",
        "        \n",
        "        ctx_len = ctx.shape[1]\n",
        "        \n",
        "        ques_len = ques.shape[1]\n",
        "        \n",
        "        ## GET WORD AND CHARACTER EMBEDDINGS\n",
        "        \n",
        "        ctx_word_embed = self.word_embedding(ctx)\n",
        "        # ctx_word_embed = [bs, ctx_len, emb_dim]\n",
        "        \n",
        "        ques_word_embed = self.word_embedding(ques)\n",
        "        # ques_word_embed = [bs, ques_len, emb_dim]        \n",
        "        ctx_char_embed = self.character_embedding(char_ctx)\n",
        "        # ctx_char_embed =  [bs, ctx_len, emb_dim]\n",
        "        \n",
        "        ques_char_embed = self.character_embedding(char_ques)\n",
        "        # ques_char_embed = [bs, ques_len, emb_dim]\n",
        "        \n",
        "        ## CREATE CONTEXTUAL EMBEDDING\n",
        "        \n",
        "        ctx_contextual_inp = torch.cat([ctx_word_embed, ctx_char_embed],dim=2)\n",
        "        # [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        ques_contextual_inp = torch.cat([ques_word_embed, ques_char_embed],dim=2)\n",
        "        # [bs, ques_len, emb_dim*2]\n",
        "        \n",
        "        ctx_contextual_emb = self.contextual_embedding(ctx_contextual_inp)\n",
        "        # [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        ques_contextual_emb = self.contextual_embedding(ques_contextual_inp)\n",
        "        # [bs, ques_len, emb_dim*2]\n",
        "        \n",
        "        \n",
        "        ## CREATE SIMILARITY MATRIX\n",
        "        \n",
        "        ctx_ = ctx_contextual_emb.unsqueeze(2).repeat(1,1,ques_len,1)\n",
        "        # [bs, ctx_len, 1, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
        "        \n",
        "        ques_ = ques_contextual_emb.unsqueeze(1).repeat(1,ctx_len,1,1)\n",
        "        # [bs, 1, ques_len, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
        "        \n",
        "        elementwise_prod = torch.mul(ctx_, ques_)\n",
        "        # [bs, ctx_len, ques_len, emb_dim*2]\n",
        "        \n",
        "        alpha = torch.cat([ctx_, ques_, elementwise_prod], dim=3)\n",
        "        # [bs, ctx_len, ques_len, emb_dim*6]\n",
        "        \n",
        "        similarity_matrix = self.similarity_weight(alpha).view(-1, ctx_len, ques_len)\n",
        "        # [bs, ctx_len, ques_len]\n",
        "        \n",
        "        \n",
        "        ## CALCULATE CONTEXT2QUERY ATTENTION\n",
        "        \n",
        "        a = F.softmax(similarity_matrix, dim=-1)\n",
        "        # [bs, ctx_len, ques_len]\n",
        "        \n",
        "        c2q = torch.bmm(a, ques_contextual_emb)\n",
        "        # [bs] ([ctx_len, ques_len] X [ques_len, emb_dim*2]) => [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        \n",
        "        ## CALCULATE QUERY2CONTEXT ATTENTION\n",
        "        \n",
        "        b = F.softmax(torch.max(similarity_matrix,2)[0], dim=-1)\n",
        "        # [bs, ctx_len]\n",
        "        \n",
        "        b = b.unsqueeze(1)\n",
        "        # [bs, 1, ctx_len]\n",
        "        \n",
        "        q2c = torch.bmm(b, ctx_contextual_emb)\n",
        "        # [bs] ([bs, 1, ctx_len] X [bs, ctx_len, emb_dim*2]) => [bs, 1, emb_dim*2]\n",
        "        \n",
        "        q2c = q2c.repeat(1, ctx_len, 1)\n",
        "        # [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        ## QUERY AWARE REPRESENTATION\n",
        "        \n",
        "        G = torch.cat([ctx_contextual_emb, c2q, \n",
        "                       torch.mul(ctx_contextual_emb,c2q), \n",
        "                       torch.mul(ctx_contextual_emb, q2c)], dim=2)        # [bs, ctx_len, emb_dim*8]\n",
        "        \n",
        "        \n",
        "        ## MODELING LAYER\n",
        "        \n",
        "        M, _ = self.modeling_lstm(G)\n",
        "        # [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        ## OUTPUT LAYER\n",
        "        \n",
        "        M2, _ = self.end_lstm(M)\n",
        "        \n",
        "        # START PREDICTION\n",
        "        \n",
        "        p1 = self.output_start(torch.cat([G,M], dim=2))\n",
        "        # [bs, ctx_len, 1]\n",
        "        \n",
        "        p1 = p1.squeeze()\n",
        "        # [bs, ctx_len]\n",
        "        \n",
        "        #p1 = F.softmax(p1, dim=-1)\n",
        "        \n",
        "        # END PREDICTION\n",
        "        \n",
        "        p2 = self.output_end(torch.cat([G, M2], dim=2)).squeeze()\n",
        "        # [bs, ctx_len, 1] => [bs, ctx_len]\n",
        "        \n",
        "        #p2 = F.softmax(p2, dim=-1)\n",
        "        \n",
        "        \n",
        "        return p1, p2\n",
        "      "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVlR1UXgdt6_"
      },
      "source": [
        "CHAR_VOCAB_DIM = len(char2idx)\n",
        "EMB_DIM = 300\n",
        "CHAR_EMB_DIM = 8\n",
        "NUM_OUTPUT_CHANNELS = 300\n",
        "KERNEL_SIZE = (8,5)\n",
        "HIDDEN_DIM = 300\n",
        "device = torch.device('cuda')\n",
        "\n",
        "model = BiDAF(CHAR_VOCAB_DIM, \n",
        "              EMB_DIM, \n",
        "              CHAR_EMB_DIM, \n",
        "              NUM_OUTPUT_CHANNELS, \n",
        "              KERNEL_SIZE, \n",
        "              HIDDEN_DIM, \n",
        "              device).to(device)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7HFwF-ncv99",
        "outputId": "c03e7bea-105d-4d9d-b676-c62acd79dcf1"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiDAF(\n",
              "  (word_embedding): Embedding(85812, 300)\n",
              "  (character_embedding): CharacterEmbeddingLayer(\n",
              "    (char_embedding): Embedding(193, 8, padding_idx=1)\n",
              "    (char_convolution): Conv2d(1, 300, kernel_size=(8, 5), stride=(1, 1))\n",
              "    (relu): ReLU()\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (contextual_embedding): ContextualEmbeddingLayer(\n",
              "    (lstm): LSTM(600, 300, batch_first=True, bidirectional=True)\n",
              "    (highway_net): HighwayNetwork(\n",
              "      (flow_layer): ModuleList(\n",
              "        (0): Linear(in_features=600, out_features=600, bias=True)\n",
              "        (1): Linear(in_features=600, out_features=600, bias=True)\n",
              "      )\n",
              "      (gate_layer): ModuleList(\n",
              "        (0): Linear(in_features=600, out_features=600, bias=True)\n",
              "        (1): Linear(in_features=600, out_features=600, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (similarity_weight): Linear(in_features=1800, out_features=1, bias=False)\n",
              "  (modeling_lstm): LSTM(2400, 300, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (output_start): Linear(in_features=3000, out_features=1, bias=False)\n",
              "  (output_end): Linear(in_features=3000, out_features=1, bias=False)\n",
              "  (end_lstm): LSTM(600, 300, batch_first=True, bidirectional=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14jmIo6idYQz"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "optimizer = optim.Adadelta(model.parameters())"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-frsgy5adeIl"
      },
      "source": [
        "def train(model, train_dataset):\n",
        "    print(\"Starting training ........\")\n",
        "   \n",
        "\n",
        "    train_loss = 0.\n",
        "    batch_count = 0\n",
        "    model.train()\n",
        "    for batch in train_dataset:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "        if batch_count % 500 == 0:\n",
        "            print(f\"Starting batch: {batch_count}\")\n",
        "        batch_count += 1\n",
        "        \n",
        "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
        "\n",
        "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
        "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n",
        "\n",
        "\n",
        "        preds = model(context, question, char_ctx, char_ques)\n",
        "\n",
        "        start_pred, end_pred = preds\n",
        "\n",
        "        s_idx, e_idx = label[:,0], label[:,1]\n",
        "\n",
        "        loss = F.cross_entropy(start_pred, s_idx) + F.cross_entropy(end_pred, e_idx)\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "pLwqIfRtqu8k",
        "outputId": "e35bdf2e-f049-4d85-b81d-a28d1de13370"
      },
      "source": [
        "train(model, train_dataset)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training ........\n",
            "Starting batch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-7603d78d81df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-79b8499fd7df>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_ques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_ques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0mchar_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_ques\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    }
  ]
}